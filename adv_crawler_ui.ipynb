{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d06b3-1508-459e-9e3f-cf3925c182ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install advertools dash dash_bootstrap_components jupyter_dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a0e2b-937d-4694-b8bd-6b1cf770fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import advertools as adv\n",
    "import pandas as pd\n",
    "from dash import Dash, dcc, html, callback, Input, Output, State\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash.dash_table import DataTable\n",
    "from dash.exceptions import PreventUpdate\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.express as px\n",
    "\n",
    "app = JupyterDash(\n",
    "    __name__,\n",
    "    suppress_callback_exceptions=True,\n",
    "    external_stylesheets=[dbc.themes.FLATLY,\n",
    "                          dbc.icons.BOOTSTRAP])\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dbc.Row([\n",
    "        dbc.Col(lg=1, md=1, sm=1),\n",
    "        dbc.Col([\n",
    "            html.Br(),\n",
    "            html.H1([html.Code('advertools'),  ' SEO Crawler']), html.Br(),\n",
    "            dbc.Label(\"Name a folder to store your crawl project's data:\"),\n",
    "            dbc.Input(id='crawl_project',\n",
    "                      pattern='[a-zA-z]\\S{5,}'),\n",
    "            dbc.Tooltip('starts with a letter, no spaces, > 4 chars. e.g. DOMAIN_YYYY_MM_DD',\n",
    "                        target='crawl_project'),\n",
    "            html.Br(),\n",
    "            dbc.Label('Enter start URL(s):'),\n",
    "            dbc.Textarea(id='start_urls', rows=4), html.Br(),\n",
    "            dbc.Tooltip('One or more URLs, one per line', target='start_urls'),\n",
    "            dbc.Checkbox(id='follow_links',label='Follow links', value=0),\n",
    "            dbc.Tooltip('Should the crawler follow and crawl links found on pages recursively? Unticking this would crawl in list mode',\n",
    "                        target='follow_links'), html.Br(),\n",
    "            dbc.Button(['Advanced options ',\n",
    "                        html.I(className='bi bi-chevron-expand')],\n",
    "                       color='light',\n",
    "                       id='open_collapse'),\n",
    "            dbc.Collapse([\n",
    "                html.Br(),\n",
    "                html.H5([\"URL Parameters \",\n",
    "                         html.I(id='url_param_question',\n",
    "                                className='bi bi-question-circle')]),\n",
    "                dbc.Tooltip(\"\"\"\n",
    "                While discovering and following links you might want to exclude\n",
    "                and/or include URLs that contain certain parameters. Enter\n",
    "                parameters separated by space, e.g.: color price country\n",
    "                \"\"\", target='url_param_question'),\n",
    "                dbc.Row([\n",
    "                    dbc.Col([\n",
    "                        dbc.Label('Exclude:'),\n",
    "                        dbc.Input(id='exclude_url_params',), html.Br(),\n",
    "                    ]),\n",
    "                    dbc.Col([\n",
    "                        dbc.Label('Include:'),\n",
    "                        dbc.Input(id='include_url_params',), html.Br(),\n",
    "                    ])\n",
    "                ]),\n",
    "                html.H5([\"URL Regex \",\n",
    "                         html.I(id='url_regex_question',\n",
    "                                className='bi bi-question-circle')]),\n",
    "                dbc.Tooltip(\"\"\"\n",
    "                While discovering and following links you might want to exclude\n",
    "                and/or include URLs that match a certain regular expression.\n",
    "                \"\"\", target='url_regex_question'),\n",
    "\n",
    "                dbc.Row([\n",
    "                    dbc.Col([\n",
    "                        dbc.Label('Exclude:'),\n",
    "                        dbc.Input(id='exclude_url_regex',), html.Br(),\n",
    "                    ]),\n",
    "                    dbc.Col([\n",
    "                        dbc.Label('Include:'),\n",
    "                        dbc.Input(id='include_url_regex',), html.Br(),\n",
    "                    ])\n",
    "                ]),\n",
    "                dbc.Label('User-agent:'),\n",
    "                dbc.Input(id='USER_AGENT'), html.Br(),\n",
    "                dbc.Label('Maximum pages to crawl:'),\n",
    "                dbc.Input(id='CLOSESPIDER_PAGECOUNT', inputmode='numeric',\n",
    "                          pattern='\\d+'),\n",
    "            ], is_open=False, id='advanced_options'), html.Br(), html.Br(),\n",
    "            dbc.Button('Start', id='crawl_start_button'), \n",
    "            dcc.Loading([\n",
    "                html.Div(id='crawl_status'),\n",
    "            ]), html.Div([html.Br()] * 20),\n",
    "        ], lg=6),\n",
    "        dbc.Col(lg=1),\n",
    "    ])\n",
    "])\n",
    "\n",
    "\n",
    "@callback(\n",
    "    Output('crawl_status', 'children'),\n",
    "    Input('crawl_start_button', 'n_clicks'),\n",
    "    State('crawl_project', 'value'),\n",
    "    State('start_urls', 'value'),\n",
    "    State('follow_links', 'value'),\n",
    "    State('USER_AGENT', 'value'),\n",
    "    State('CLOSESPIDER_PAGECOUNT', 'value'),\n",
    "    State('exclude_url_params', 'value'),\n",
    "    State('include_url_params', 'value'),\n",
    "    State('exclude_url_regex', 'value'),\n",
    "    State('include_url_regex', 'value'))\n",
    "def start_crawling(\n",
    "    n_clicks, crawl_project, start_urls,\n",
    "    follow_links, USER_AGENT, CLOSESPIDER_PAGECOUNT, \n",
    "    exclude_url_params, include_url_params, exclude_url_regex,\n",
    "    include_url_regex):\n",
    "    if not n_clicks or not start_urls or not crawl_project:\n",
    "        raise PreventUpdate\n",
    "    try:\n",
    "        os.mkdir(crawl_project)\n",
    "    except FileExistsError:\n",
    "        return html.Br(), html.Br(), html.Div([\n",
    "            \"\"\"Seems this folder already exists. Either move it, or select\n",
    "            a different name\"\"\"\n",
    "        ])\n",
    "    url_list = [x.strip() for x in start_urls.splitlines()]\n",
    "    if exclude_url_params is not None:\n",
    "        exclude_url_params = exclude_url_params.split()\n",
    "    if include_url_params is not None:\n",
    "        include_url_params = include_url_params.split()\n",
    "\n",
    "    adv.crawl(\n",
    "        url_list,\n",
    "        f'{crawl_project}/crawl.jl',\n",
    "        follow_links=bool(follow_links),\n",
    "        exclude_url_params=exclude_url_params,\n",
    "        include_url_params=include_url_params,\n",
    "        exclude_url_regex=exclude_url_regex,\n",
    "        include_url_regex=include_url_regex,\n",
    "        custom_settings={\n",
    "            'JOBDIR': f'{crawl_project}/crawl_job.jl',\n",
    "            'LOG_FILE': f'{crawl_project}/crawl_logs.log',\n",
    "            'USER_AGENT': USER_AGENT or adv.spider.user_agent,\n",
    "            'CLOSESPIDER_PAGECOUNT': CLOSESPIDER_PAGECOUNT or 0,\n",
    "        })\n",
    "    crawl_df = pd.read_json(f'{crawl_project}/crawl.jl', lines=True)\n",
    "    return html.Div([\n",
    "        html.Br(),\n",
    "        html.H2(\"Crawl dataset (sample rows):\"), html.Br(),\n",
    "        dbc.Button('Export', id='export_button'), html.Br(), html.Br(),\n",
    "        dcc.Download(id=\"download_crawldf\"),\n",
    "        DataTable(\n",
    "            data=crawl_df.head(50).astype(str).to_dict('records'),\n",
    "            columns=[{\"name\": i, \"id\": i} for i in crawl_df.columns],\n",
    "            fixed_rows={'headers': True},\n",
    "             style_header={\n",
    "                 'fontFamily': 'Arial',\n",
    "                 'fontColor': '#2F3B4C',\n",
    "                 'fontWeight': 'bold'},\n",
    "            style_data={'fontFamily': 'Arial'},\n",
    "            style_cell={\n",
    "                'overflow': 'hidden',\n",
    "                'textOverflow': 'ellipsis',\n",
    "                'maxWidth': 200})\n",
    "    ])\n",
    "\n",
    "\n",
    "@callback(Output('download_crawldf', 'data'),\n",
    "          State('crawl_project', 'value'),\n",
    "          Input('export_button', 'n_clicks'),\n",
    "          prevent_initial_call=True)\n",
    "def export_crawl_df(crawl_project, n_clicks):\n",
    "    full_crawl_df = pd.read_json(f'{crawl_project}/crawl.jl', lines=True)\n",
    "    return dcc.send_data_frame(full_crawl_df.to_excel, f\"{crawl_project}.csv\", index=False)\n",
    "\n",
    "\n",
    "@callback(Output('advanced_options', 'is_open'),\n",
    "              Input('open_collapse', 'n_clicks'),\n",
    "              State('advanced_options', 'is_open'))\n",
    "def toggle_collapse(n_clicks, is_open):\n",
    "    if n_clicks:\n",
    "        return not is_open\n",
    "    return is_open\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
